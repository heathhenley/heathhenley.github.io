<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>applied math on Blogging by Heath™</title><link>https://heathhenley.github.io/categories/applied-math/</link><description>Recent content in applied math on Blogging by Heath™</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 26 May 2024 00:15:12 -0400</lastBuildDate><atom:link href="https://heathhenley.github.io/categories/applied-math/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes: gradient descent, Newton-Raphson, Lagrange Multipliers</title><link>https://heathhenley.github.io/posts/numerical-methods/</link><pubDate>Sun, 26 May 2024 00:15:12 -0400</pubDate><guid>https://heathhenley.github.io/posts/numerical-methods/</guid><description>TL;DR: A quick &amp;ldquo;non-mathematical&amp;rdquo; introduction to the most basic forms of gradient descent and Newton-Raphson methods to solve optimization problems involving functions of more than one variable. We also look at the Lagrange Multiplier method to solve optimization problems subject to constraints (and what the resulting system of nonlinear equations looks like, eg what we could apply Newton-Raphson to, etc).
Introduction Optimization problems are everywhere in engineering and science. If you can model your problem in a way that can write down some function that should be minimized or maximized (the objective function) to get the solution you want, even in cases where there is no analytical solution (most real cases), you can often obtain a numerical solution.</description></item></channel></rss>