<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>optimization on Blogging by Heath™</title><link>https://heathhenley.dev/tags/optimization/</link><description>Recent content in optimization on Blogging by Heath™</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 20 Dec 2025 12:17:28 -0500</lastBuildDate><atom:link href="https://heathhenley.dev/tags/optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>Simple query refactor - 100x faster</title><link>https://heathhenley.dev/posts/pg-tuple-compare-switch/</link><pubDate>Sat, 20 Dec 2025 12:17:28 -0500</pubDate><guid>https://heathhenley.dev/posts/pg-tuple-compare-switch/</guid><description>TL;DR - Recently set out to speed up a slow query behind a multi-column cursor paginated endpoint. A simple switch to represent the where filters as a tuple makes a huge difference in performance on the exact same data with the same indices. Eg - change where ( a &amp;gt; c or (a = c and b &amp;gt; d )) to (a, b) &amp;gt; (c, d). In the tuple case postgres can use the index more efficiently and get more of the needed rows using the index condition, versus walking it, reading rows in and filtering.</description></item><item><title>Notes: Gradient Descent, Newton-Raphson, Lagrange Multipliers</title><link>https://heathhenley.dev/posts/numerical-methods/</link><pubDate>Sun, 26 May 2024 00:15:12 -0400</pubDate><guid>https://heathhenley.dev/posts/numerical-methods/</guid><description>TL;DR: A quick &amp;ldquo;non-mathematical&amp;rdquo; introduction to the most basic forms of gradient descent and Newton-Raphson methods to solve optimization problems involving functions of more than one variable. We also look at the Lagrange Multiplier method to solve optimization problems subject to constraints (and what the resulting system of nonlinear equations looks like, eg what we could apply Newton-Raphson to, etc).
Introduction Optimization problems are everywhere in engineering and science. If you can model your problem in a way that can write down some function that should be minimized or maximized (the objective function) to get the solution you want, even in cases where there is no analytical solution (most real cases), you can often obtain a numerical solution.</description></item><item><title>Memoization in the Wild</title><link>https://heathhenley.dev/posts/memoization-in-the-wild/</link><pubDate>Mon, 02 Aug 2021 17:32:43 -0400</pubDate><guid>https://heathhenley.dev/posts/memoization-in-the-wild/</guid><description>Overview Memoization or memoisation is a method used to optimize programs. Usually, at least in my experience, it’s one of the first topics introduced when dynamic programming algorithms are being discussed. With a quick google search you can find the Wiki or a trillion other blogs about it - most will show the canonical example - the “hello world” of the topic - that is, using memoization to optimize a recursive implementation of a function that generates the n-th Fibonacci number (or sometimes a function computing factorials).</description></item></channel></rss>